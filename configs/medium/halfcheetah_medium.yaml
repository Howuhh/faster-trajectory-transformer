wandb:
  project: "offlineRL" # write yours
  name: "halfcheetah_medium"
  group: "gpt_cache"
  entity: "matthew-cy" # write yours
  mode: "online"  # set to online, if needed

dataset:
  env_name: "halfcheetah-medium-v2"
  seq_len: 10
  cache_path: "data"
  num_bins: ${model.vocab_size}
  discount: 0.99
  strategy: "uniform"
  batch_size: 256

model:
  vocab_size: 100
  transition_dim: 25
  observation_dim: 17
  action_dim: 6
  seq_len: 250
  embedding_dim: 128
  num_layers: 4
  num_heads: 4
  use_sep_heads: true

trainer:
  num_epochs_ref: 50
  action_weight: 5
  value_weight: 1
  reward_weight: 1
  lr: 6e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1
  clip_grad: 1.0
  eval_seed: 42
  eval_every: 10
  eval_episodes: 5
  eval_discount: ${dataset.discount}
  eval_temperature: 1
  eval_plan_every: 1
  eval_beam_width: 32
  eval_beam_steps: 5
  eval_beam_context: 5
  eval_sample_expand: 2
  eval_k_obs: 1
  eval_k_reward: 1
  eval_k_act: null
  checkpoints_path: "checkpoints/${dataset.env_name}/${dataset.strategy}/baseline"
